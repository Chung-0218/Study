{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32fe2b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_dir DATA_DIR] --model MODEL\n",
      "                             --weights WEIGHTS [--batch_size BATCH_SIZE]\n",
      "                             [--gpu]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model, --weights\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import utils\n",
    "import data_loader\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', default='./data/test',\n",
    "                    help=\"Directory containing the dataset\")\n",
    "parser.add_argument('--model', type=str, required=True,\n",
    "                    help=\"The model you want to test\")\n",
    "parser.add_argument('--weights', required=True,\n",
    "                    help=\"The weights file you want to test\")\n",
    "parser.add_argument('--batch_size', default=256,\n",
    "                    help=\"batch size\")\n",
    "parser.add_argument('--gpu', action='store_true', default='False',\n",
    "                    help=\"GPU available\")\n",
    "\n",
    "\n",
    "def evaluate(model, loss_fn, dataloader):\n",
    "    \"\"\" Evaluate the model on `num_steps` batches\n",
    "    Args:\n",
    "        model : (torch.nn.Module) model\n",
    "        dataloader : (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n",
    "        num_steps : (int) # of batches to train on, each of size args.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to test mode\n",
    "    model.eval()\n",
    "\n",
    "    model_dir = './results/' + model_name\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0.0\n",
    "\n",
    "    for i, (test_batch, labels_batch) in enumerate(dataloader):\n",
    "        # move to GPU if available\n",
    "        if args.gpu:\n",
    "            test_batch, labels_batch = test_batch.cuda(), labels_batch.cuda()\n",
    "\n",
    "        # convert to torch Variable\n",
    "        test_batch, labels_batch = Variable(test_batch), Variable(labels_batch)\n",
    "\n",
    "        # compute model output and loss\n",
    "        output_batch = model(test_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        acc = utils.accuracy(output_batch.data.cpu().numpy(), labels_batch.data.cpu().numpy())\n",
    "        total_correct += acc\n",
    "\n",
    "    print(\"Loss:{:.4f}\\t Test Accuracy:{:.4f}\".format(\n",
    "        total_loss/len(dataloader),\n",
    "        100 * total_correct / len(dataloader)\n",
    "    ))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Load the parameters from parser\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model_name = args.model\n",
    "    weights_path = args.weights\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    logging.info(\"Loading the test dataset...\")\n",
    "\n",
    "    # fetch train dataloader\n",
    "    test_dataloader = data_loader.test_data_loader()\n",
    "\n",
    "    logging.info(\"- done.\")\n",
    "\n",
    "    # Define the model and optimizer\n",
    "    model = utils.get_network(args)\n",
    "    checkpoint = torch.load(weights_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    # fetch loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    logging.info(\"Starting Test ...\")\n",
    "    evaluate(model, loss_fn, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88624313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def get_network(args):\n",
    "    \"\"\" Return the given network\n",
    "    Args:\n",
    "        args : (argparser)\n",
    "    \"\"\"\n",
    "\n",
    "    if args.model == 'alexnet':\n",
    "        from models.alexnet import alexnet\n",
    "        net = alexnet()\n",
    "    if args.model == 'zfnet':\n",
    "        from models.ZFNet import ZFNet\n",
    "        net = ZFNet()\n",
    "    # elif args.net == 'vgg':\n",
    "    #     from models.vgg import vgg\n",
    "    #     net = vgg()\n",
    "    # elif ...\n",
    "    #       ...\n",
    "\n",
    "    else:\n",
    "        print('the network name you have entered is not supported yet')\n",
    "        sys.exit()\n",
    "\n",
    "    if args.gpu:\n",
    "        net = net.cuda()\n",
    "\n",
    "    return net\n",
    "\n",
    "def get_optimizer(model_name, model, lr):\n",
    "    \"\"\" Return the optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    if model_name == 'alexnet':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif model_name == 'zfnet':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum = 0.9)\n",
    "    # elif args.net == 'vgg':\n",
    "    #     from models.vgg import vgg\n",
    "    #     net = vgg()\n",
    "    # elif ...\n",
    "    #       ...\n",
    "\n",
    "    else:\n",
    "        print('the network name you have entered is not supported yet')\n",
    "        sys.exit()\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    \"\"\" Compute the accuracy\n",
    "    Args:\n",
    "        outputs : (nd.ndarray) prediction\n",
    "        labels : (nd.ndarray) real\n",
    "    Returns:\n",
    "          (float) accuaracy in [0,1]\n",
    "    \"\"\"\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "    return np.sum(outputs==labels)/float(labels.size)\n",
    "\n",
    "def save_checkpoints(state, is_best, checkpoint):\n",
    "    \"\"\" Saves model and training parameters at checkpoint + 'last.pth.\n",
    "    If is_best==True, also saves checkpoint + 'best.pth'\n",
    "\n",
    "    Args:\n",
    "        state : (dict) contains model's state_dict\n",
    "        is_best : (bool) True if it is the best model\n",
    "        checkpoint : (string) folder where parameters are to be saved\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(checkpoint, 'last.pth')\n",
    "    if not os.path.exists(checkpoint):\n",
    "        print(\"Checkpoint Direcotry doesn't exist. Making directory {}\".format(checkpoint))\n",
    "        os.mkdir(checkpoint)\n",
    "    else:\n",
    "        pass\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf6aba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_dir DATA_DIR] --model MODEL\n",
      "                             [--lr LR] [--epoch EPOCH]\n",
      "                             [--batch_size BATCH_SIZE] [--gpu]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import utils\n",
    "import data_loader\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', default='./data/train',\n",
    "                    help=\"Directory containing the dataset\")\n",
    "parser.add_argument('--model', type=str, required=True,\n",
    "                    help=\"The model you want to train\")\n",
    "parser.add_argument('--lr', type=float, default=0.001,\n",
    "                    help=\"Learning rate\")\n",
    "parser.add_argument('--epoch', type=int, default=50,\n",
    "                    help=\"Total training epochs\")\n",
    "parser.add_argument('--batch_size', type=int, default=256,\n",
    "                    help=\"batch size\")\n",
    "parser.add_argument('--gpu', action='store_true', default='False',\n",
    "                    help=\"GPU available\")\n",
    "\n",
    "\n",
    "def train(model, optimizer, loss_fn, dataloader):\n",
    "    \"\"\" Train the model on `num_steps` batches\n",
    "    Args:\n",
    "        model : (torch.nn.Module) model\n",
    "        optimizer : (torch.optim) optimizer for parameters of model\n",
    "        loss_fn : (string) a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        dataloader : (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n",
    "        num_steps : (int) # of batches to train on, each of size args.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    model_dir = './results/' + model_name\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_correct = 0.0\n",
    "\n",
    "        for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
    "            # move to GPU if available\n",
    "            if args.gpu:\n",
    "                train_batch, labels_batch = train_batch.cuda(), labels_batch.cuda()\n",
    "\n",
    "            # convert to torch Variable\n",
    "            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)\n",
    "\n",
    "            # compute model output and loss\n",
    "            output_batch = model(train_batch)\n",
    "            loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "\n",
    "            # clear previous gradients, compute gradients of all variables wrt loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # performs updates using calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            acc = utils.accuracy(output_batch.data.cpu().numpy(), labels_batch.data.cpu().numpy())\n",
    "            epoch_correct += acc\n",
    "\n",
    "#             print(\"Epoch [{}]\\t Batch [{}/{}]\\t Loss:{:.4f}\\t Accuracy:{:.4f}\".format(epoch+1, i, len(dataloader), loss.item(), acc))\n",
    "\n",
    "        print(\"Epoch [{}/{}]\\t Loss:{:.4f}\\t Accuracy:{:.4f}%\".format(\n",
    "            epoch + 1,\n",
    "            epochs,\n",
    "            epoch_loss/len(dataloader),\n",
    "            100 * epoch_correct / len(dataloader)\n",
    "        ))\n",
    "\n",
    "        is_best = acc >= best_acc\n",
    "        if is_best:\n",
    "            logging.info(\"- Found new best accuracy\")\n",
    "            best_acc = acc\n",
    "\n",
    "        utils.save_checkpoints(\n",
    "            {'epoch': i + 1,\n",
    "             'state_dict': model.state_dict(),\n",
    "             'optim_dict': optimizer.state_dict()},\n",
    "            is_best=is_best,\n",
    "            checkpoint=model_dir\n",
    "        )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Load the parameters from parser\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model_name = args.model\n",
    "    lr = args.lr\n",
    "    epochs = args.epoch\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    logging.info(\"Loading the training dataset...\")\n",
    "\n",
    "    # fetch train dataloader\n",
    "    train_dataloader = data_loader.train_data_loader()\n",
    "\n",
    "    logging.info(\"- done.\")\n",
    "\n",
    "    # Define the model and optimizer\n",
    "    model = utils.get_network(args)\n",
    "    optimizer = utils.get_optimizer(model_name, model, lr)\n",
    "\n",
    "    # fetch loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    logging.info(\"Starting training for {} epoch(s).\".format(epochs))\n",
    "    train(model, optimizer, loss_fn, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67894e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "def train_data_loader(batch_size=256, workers=1, shuffle=True):\n",
    "    \"\"\" return training, test dataloader\n",
    "    Args:\n",
    "        batch_size : (int) dataloader batchsize\n",
    "        workers : (int) # of subprocesses\n",
    "        shuffle : (bool) data shuffle at every epoch\n",
    "    Returns:\n",
    "        train_data_loader : torch dataloader obj.\n",
    "        test_data_loader : torch dataloader obj.\n",
    "    \"\"\"\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.4913997551666284, 0.48215855929893703, 0.4465309133731618],\n",
    "                             std = [0.24703225141799082, 0.24348516474564, 0.26158783926049628])\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data/train', train=True, download=True, transform=transform)\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=workers\n",
    "    )\n",
    "\n",
    "    return train_data_loader\n",
    "\n",
    "def test_data_loader(batch_size=256, workers=1, shuffle=True):\n",
    "    \"\"\" return training, test dataloader\n",
    "    Args:\n",
    "        batch_size : (int) dataloader batchsize\n",
    "        workers : (int) # of subprocesses\n",
    "        shuffle : (bool) data shuffle at every epoch\n",
    "    Returns:\n",
    "        train_data_loader : torch dataloader obj.\n",
    "        test_data_loader : torch dataloader obj.\n",
    "    \"\"\"\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.4913997551666284, 0.48215855929893703, 0.4465309133731618],\n",
    "                             std = [0.24703225141799082, 0.24348516474564, 0.26158783926049628])\n",
    "    ])\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(root='./data/test', train=False, download=True, transform=transform)\n",
    "    test_data_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=workers\n",
    "    )\n",
    "\n",
    "    return test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd0c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ZFNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ZFNet, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # layer 1\n",
    "            nn.Conv2d(3, 96, kernel_size=7, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Local contrast norm.이 있어야 하는데 파이토치에는 해당 클래스가 없는 듯? LocalResponseNorm 과는 다른 건가?\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1, return_indices=True), # return_indices – if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later\n",
    "\n",
    "            # layer 2\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1, return_indices=True),\n",
    "\n",
    "            # layer 3\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # layer 4\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # layer 5\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, return_indices=True),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            # layer 6\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            # layer 7\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "        self.feature_outputs = [0]*len(self.features)\n",
    "        self.switch_indices = dict()\n",
    "        self.sizes = dict()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for i, layer in enumerate(self.features):\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                x, indices = layer(x)\n",
    "                self.feature_outputs[i] = x\n",
    "                self.switch_indices[i] = indices\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                self.feature_outputs[i] = x\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def alexnet(**kwargs):\n",
    "    model = ZFNet(**kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f863af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
